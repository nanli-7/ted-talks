{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv('transcripts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good morning. How are you?(Laughter)It's been ...</td>\n",
       "      <td>https://www.ted.com/talks/ken_robinson_says_sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
       "      <td>https://www.ted.com/talks/al_gore_on_averting_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
       "      <td>https://www.ted.com/talks/david_pogue_says_sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you're here today — and I'm very happy that...</td>\n",
       "      <td>https://www.ted.com/talks/majora_carter_s_tale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>About 10 years ago, I took on the task to teac...</td>\n",
       "      <td>https://www.ted.com/talks/hans_rosling_shows_t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  Good morning. How are you?(Laughter)It's been ...   \n",
       "1  Thank you so much, Chris. And it's truly a gre...   \n",
       "2  (Music: \"The Sound of Silence,\" Simon & Garfun...   \n",
       "3  If you're here today — and I'm very happy that...   \n",
       "4  About 10 years ago, I took on the task to teac...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ted.com/talks/ken_robinson_says_sc...  \n",
       "1  https://www.ted.com/talks/al_gore_on_averting_...  \n",
       "2  https://www.ted.com/talks/david_pogue_says_sim...  \n",
       "3  https://www.ted.com/talks/majora_carter_s_tale...  \n",
       "4  https://www.ted.com/talks/hans_rosling_shows_t...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transcript    object\n",
       "url           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted = pandas.read_csv('ted_main.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The tokenize method breaks raw strings into words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'morning', '.', 'How', 'are', 'you', '?(', 'Laughter', ')', 'It', \"'\", 's', 'been', 'great', ',', 'hasn', \"'\", 't', 'it', '?', 'I', \"'\", 've', 'been', 'blown', 'away', 'by', 'the', 'whole', 'thing', '.', 'In', 'fact', ',', 'I', \"'\", 'm', 'leaving', '.(', 'Laughter', ')', 'There', 'have', 'been', 'three', 'themes', 'running', 'through', 'the', 'conference', 'which', 'are', 'relevant', 'to', 'what', 'I', 'want', 'to', 'talk', 'about', '.', 'One', 'is', 'the', 'extraordinary', 'evidence', 'of', 'human', 'creativity', 'in', 'all', 'of', 'the', 'presentations', 'that', 'we', \"'\", 've', 'had', 'and', 'in', 'all', 'of', 'the', 'people', 'here', '.', 'Just', 'the', 'variety', 'of', 'it', 'and', 'the', 'range', 'of', 'it', '.', 'The', 'second']\n"
     ]
    }
   ],
   "source": [
    "# take test function for the first record\n",
    "sample_word_tokens = nltk.wordpunct_tokenize(df.transcript[0])\n",
    "print(sample_word_tokens[:100]) # print first 100 words from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to the entire datasets\n",
    "df['word_tokens'] = df.transcript.apply(lambda x: nltk.wordpunct_tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Normalization generally refers to a series of related tasks meant to put all text on a level playing field: converting all text to the same case (upper or lower), removing punctuation, converting numbers to their word equivalents, and so on.\n",
    "\n",
    "* If the token is a stopword or if every character is punctuation, the token is ignored. If it is not ignored, the part of speech is used to lemmatize the token, which is then yielded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to normalize words from list of tokenized words\"\"\"\n",
    "def normalize(list_of_tokens):\n",
    "    new_word_list = []\n",
    "    for word in list_of_tokens:\n",
    "        \n",
    "        # remove non-ASCII characters from list\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        \n",
    "        # Convert all characters to lowercase \n",
    "        new_word = new_word.lower() \n",
    "        \n",
    "        new_word = new_word.strip('—')\n",
    "        \n",
    "        # Replace all interger occurrences with textual representation\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "        \n",
    "        # If stopword, ignore token and continue\n",
    "        if new_word in stopwords:\n",
    "            continue\n",
    "            \n",
    "        # If punctuation, ignore token and continue\n",
    "        if all(char in punct for char in new_word):\n",
    "            continue\n",
    "            \n",
    "        new_word_list.append(new_word)\n",
    "        \n",
    "    return new_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'morning', 'laughter', 'great', 'blown', 'away', 'whole', 'thing', 'fact', 'leaving', 'laughter', 'three', 'themes', 'running', 'conference', 'relevant', 'want', 'talk', 'one', 'extraordinary', 'evidence', 'human', 'creativity', 'presentations', 'people', 'variety', 'range', 'second', 'put', 'us', 'place', 'idea', 'going', 'happen', 'terms', 'future', 'idea', 'may', 'play', 'interest', 'education', 'actually', 'find', 'everybody', 'interest', 'education', 'find', 'interesting', 'dinner', 'party', 'say', 'work', 'education', 'actually', 'often', 'dinner', 'parties', 'frankly', 'laughter', 'work', 'education', 'asked', 'laughter', 'never', 'asked', 'back', 'curiously', 'strange', 'say', 'somebody', 'know', 'say', 'say', 'work', 'education', 'see', 'blood', 'run', 'face', 'like', 'oh', 'god', 'know', 'laughter', 'one', 'night', 'week', 'laughter', 'ask', 'education', 'pin', 'wall', 'one', 'things', 'goes', 'deep', 'people', 'right', 'like', 'religion']\n"
     ]
    }
   ],
   "source": [
    "print(normalize(sample_word_tokens)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['normalize_tokens'] = df.word_tokens.apply(lambda x: normalize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('laughter', 39),\n",
       " ('think', 26),\n",
       " ('education', 22),\n",
       " ('said', 22),\n",
       " ('people', 16),\n",
       " ('one', 14),\n",
       " ('know', 11),\n",
       " ('say', 10),\n",
       " ('things', 10),\n",
       " ('want', 9),\n",
       " ('like', 9),\n",
       " ('school', 9),\n",
       " ('way', 9),\n",
       " ('children', 8),\n",
       " ('really', 8),\n",
       " ('whole', 7),\n",
       " ('human', 7),\n",
       " ('world', 7),\n",
       " ('kids', 7),\n",
       " ('went', 7),\n",
       " ('come', 7),\n",
       " ('wrong', 7),\n",
       " ('system', 7),\n",
       " ('gillian', 7),\n",
       " ('thing', 6),\n",
       " ('future', 6),\n",
       " ('actually', 6),\n",
       " ('work', 6),\n",
       " ('never', 6),\n",
       " ('get', 6)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(df.normalize_tokens[0]).most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lemmatization is the process of looking up a single word form from the variety of morphologic affixes that can be applied to indicate tense, plurality, gender, etc. First we need to identify the WordNet tag form based on the Penn Treebank tag, which is returned from NLTK’s standard pos_tag function. We simply look to see if the Penn tag starts with ‘N’, ‘V’, ‘R’, or ‘J’ and can correctly identify if its a noun, verb, adverb, or adjective. We then use the new tag to look up the lemma in the lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordNetLemmatizer looks up data from the WordNet lexicon.\n",
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn= nltk.corpus.wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test function for first record\n",
    "lemma_list=[]\n",
    "for token, tag in nltk.pos_tag(df.normalize_tokens[0]):\n",
    "    tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "    lemma = lemmatizer.lemmatize(token, pos=tag)\n",
    "    lemma_list.append(lemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('laughter', 39),\n",
       " ('say', 32),\n",
       " ('think', 27),\n",
       " ('education', 22),\n",
       " ('thing', 16),\n",
       " ('people', 16),\n",
       " ('go', 15),\n",
       " ('one', 14),\n",
       " ('know', 14),\n",
       " ('get', 12),\n",
       " ('come', 12),\n",
       " ('way', 11),\n",
       " ('school', 10),\n",
       " ('want', 9),\n",
       " ('see', 9),\n",
       " ('like', 9),\n",
       " ('child', 9),\n",
       " ('kid', 9),\n",
       " ('system', 9),\n",
       " ('talk', 8),\n",
       " ('year', 8),\n",
       " ('really', 8),\n",
       " ('whole', 7),\n",
       " ('human', 7),\n",
       " ('world', 7),\n",
       " ('wrong', 7),\n",
       " ('gillian', 7),\n",
       " ('future', 6),\n",
       " ('actually', 6),\n",
       " ('work', 6)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the result\n",
    "collections.Counter(lemma_list).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap-up into function and apply to entire data\n",
    "def lemmantize_tokens(list_of_tokens):\n",
    "    lemma_list=[]\n",
    "    for token, tag in nltk.pos_tag(list_of_tokens):\n",
    "        tag = {\n",
    "                'N': wn.NOUN,\n",
    "                'V': wn.VERB,\n",
    "                'R': wn.ADV,\n",
    "                'J': wn.ADJ\n",
    "            }.get(tag[0], wn.NOUN)\n",
    "        lemma = lemmatizer.lemmatize(token, pos=tag)\n",
    "        lemma_list.append(lemma)\n",
    "    return lemma_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized_tokens\"] = df.normalize_tokens.apply(lambda x : lemmantize_tokens(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(df.lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save('transcripts.dict') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(49874 unique tokens: ['1930s', '19th', '30', 'ability', 'abstract']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in df.lemmatized_tokens]\n",
    "gensim.corpora.MmCorpus.serialize('transcripts.mm', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating corpus transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model\n",
    "tfidf = gensim.models.TfidfModel(corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transformation to entire corpus\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Similarity Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.030237824410662306), (1, 0.024437734326404555), (2, 0.030940817578001496), (3, 0.02277810061334567), (4, 0.023419616787144208), (5, 0.09313904396824561), (6, 0.018287906824639204), (7, 0.024437734326404555), (8, 0.014056565701431082), (9, 0.07922979815394544)]\n"
     ]
    }
   ],
   "source": [
    "# find most relevant ted talk to the first talk,  \n",
    "# sort our corpus documents in decreasing order of relevance to this query\n",
    "\n",
    "vec_bow = dictionary.doc2bow(df.lemmatized_tokens[0])\n",
    "vec_tfidf = tfidf[vec_bow]  # convert the query to tf-idf space\n",
    "print(vec_tfidf[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Step 1: initializing query structures__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = gensim.similarities.MatrixSimilarity(corpus_tfidf)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Step 2: Performing queries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = index[vec_tfidf]  # perform a similarity query against the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted (document number, similarity score) 2-tuples\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1421, 0.24826032),\n",
       " (663, 0.18575576),\n",
       " (1968, 0.16116989),\n",
       " (685, 0.15902457),\n",
       " (1711, 0.15880892),\n",
       " (2235, 0.1460809),\n",
       " (1362, 0.14587297),\n",
       " (1358, 0.14247915),\n",
       " (1372, 0.13665263)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 related talks\n",
    "sims[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('people', 30),\n",
       " ('laughter', 29),\n",
       " ('education', 26),\n",
       " ('get', 22),\n",
       " ('know', 17),\n",
       " ('child', 16),\n",
       " ('school', 15),\n",
       " ('learn', 14),\n",
       " ('system', 12),\n",
       " ('america', 11),\n",
       " ('applause', 11),\n",
       " ('go', 11),\n",
       " ('teacher', 11),\n",
       " ('kid', 10),\n",
       " ('like', 10),\n",
       " ('one', 10),\n",
       " ('say', 10),\n",
       " ('culture', 10),\n",
       " ('life', 9),\n",
       " ('think', 8),\n",
       " ('thing', 8),\n",
       " ('country', 8),\n",
       " ('leave', 8),\n",
       " ('human', 8),\n",
       " ('teach', 8),\n",
       " ('finland', 8),\n",
       " ('want', 7),\n",
       " ('behind', 7),\n",
       " ('work', 7),\n",
       " ('condition', 7)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check top words in most related talk\n",
    "collections.Counter(df.lemmatized_tokens[1421]).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
